{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image paths have been updated in the CSV file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file = \"fd.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Define the possible extensions to try if an image is not found\n",
    "extensions_to_try = ['jpg', 'jpeg', 'webp', 'png']\n",
    "\n",
    "# Function to open the image with various extensions if needed\n",
    "def open_image_with_extensions(img_path):\n",
    "    # Try to open the image with the given path\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            return img_path  # Return the original path if the image is found\n",
    "    except FileNotFoundError:\n",
    "        # If file is not found, try the alternate extensions\n",
    "        base_path, original_ext = os.path.splitext(img_path)\n",
    "        for ext in extensions_to_try:\n",
    "            new_path = f\"{base_path}.{ext}\"\n",
    "            try:\n",
    "                with Image.open(new_path) as img:\n",
    "                    return new_path  # Return the modified path if the image is found\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        print(f\"Image not found with any of the extensions: {img_path}\")\n",
    "        return None  # Return None if image is not found with any extension\n",
    "\n",
    "# Apply the function to each img_path in the DataFrame\n",
    "df['img_path'] = df['img_path'].apply(open_image_with_extensions)\n",
    "\n",
    "# Save the updated DataFrame to the same CSV file\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(\"Image paths have been updated in the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file has been created at dlfd.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Load CSV file\n",
    "csv_file_path = 'fd.csv'  # replace with your file path\n",
    "json_file_path = 'dlfd.json'  # output JSON file\n",
    "\n",
    "# Initialize list to store JSON data\n",
    "data = []\n",
    "\n",
    "# Read CSV file\n",
    "with open(csv_file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    \n",
    "    for row in reader:\n",
    "        # For each row, create the structure as per the desired JSON format\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"content\": \"<image>Describe the image.\",\n",
    "                    \"role\": \"user\"\n",
    "                },\n",
    "                {\n",
    "                    \"content\": row[\"caption\"],  # caption from CSV\n",
    "                    \"role\": \"assistant\"\n",
    "                }\n",
    "            ],\n",
    "            \"images\": [\n",
    "                row[\"img_path\"]  # image path from CSV\n",
    "            ]\n",
    "        }\n",
    "        # Append each entry to the data list\n",
    "        data.append(entry)\n",
    "\n",
    "# Write JSON data to file\n",
    "with open(json_file_path, mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "\n",
    "print(f\"JSON file has been created at {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved monster to /teamspace/studios/this_studio/dlpro\n",
      "Moved Family to /teamspace/studios/this_studio/dlpro\n",
      "Moved Person to /teamspace/studios/this_studio/dlpro\n",
      "Moved house to /teamspace/studios/this_studio/dlpro\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_folders_to_dlpro(base_path='.'):\n",
    "    \"\"\"Creates a 'dlpro' folder and moves existing subfolders into it.\"\"\"\n",
    "    dlpro_path = os.path.join(base_path, 'dlpro')\n",
    "    subfolders = ['monster', 'Family', 'Person', 'house']\n",
    "    \n",
    "    # Create the 'dlpro' folder if it doesn't exist\n",
    "    os.makedirs(dlpro_path, exist_ok=True)\n",
    "    \n",
    "    # Move the existing subfolders into 'dlpro'\n",
    "    for folder in subfolders:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "            # Move the subfolder to 'dlpro'\n",
    "            shutil.move(folder_path, os.path.join(dlpro_path, folder))\n",
    "            print(f\"Moved {folder} to {dlpro_path}\")\n",
    "        else:\n",
    "            print(f\"{folder} does not exist in the specified base path.\")\n",
    "    \n",
    "# Usage\n",
    "move_folders_to_dlpro('/teamspace/studios/this_studio')  # Replace with the base directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n",
      "\n",
      "\n",
      "\u001b[38;5;57m\u001b[1m⚡️ Tip\u001b[0m\tConnect GitHub to Studios: \u001b[4mhttps://lightning.ai/gamerjoel001/home?settings=integrations\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /home/zeus/miniconda3/envs/cloudspace/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /home/zeus/miniconda3/envs/cloudspace/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /home/zeus/miniconda3/envs/cloudspace/bin/python -m pip install [options] [-e] <vcs project url> ...\n",
      "  /home/zeus/miniconda3/envs/cloudspace/bin/python -m pip install [options] [-e] <local project path> ...\n",
      "  /home/zeus/miniconda3/envs/cloudspace/bin/python -m pip install [options] <archive url/path> ...\n",
      "\n",
      "--find-links option requires 1 argument\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.2.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121 --find-links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: triton in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton) (3.16.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install triton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers<=4.46.1,>=4.41.2 (from -r requirements.txt (line 1))\n",
      "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: datasets<=3.1.0,>=2.16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.9.6)\n",
      "Requirement already satisfied: gradio<5.0.0,>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.44.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.1.4)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
      "Requirement already satisfied: einops in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.8.0)\n",
      "Requirement already satisfied: protobuf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.32.0)\n",
      "Requirement already satisfied: pydantic in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.9.2)\n",
      "Requirement already satisfied: fastapi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.115.4)\n",
      "Requirement already satisfied: sse-starlette in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.1.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.8.2)\n",
      "Requirement already satisfied: fire in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.7.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (24.1)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (1.26.4)\n",
      "Requirement already satisfied: av in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (13.1.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (4.66.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (3.10.10)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (6.1.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.5.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5)) (0.8.14)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (4.6.2.post1)\n",
      "Requirement already satisfied: ffmpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.27.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.10.11)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (10.4.0)\n",
      "Requirement already satisfied: pydub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.0.17)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.7.3)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: click>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (2.23.4)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (0.41.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.2.0)\n",
      "Requirement already satisfied: termcolor in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fire->-r requirements.txt (line 18)) (2.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (13.9.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5)) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.18.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "Using cached transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0.dev0\n",
      "    Uninstalling transformers-4.47.0.dev0:\n",
      "      Successfully uninstalled transformers-4.47.0.dev0\n",
      "Successfully installed transformers-4.46.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-9h7drarw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-9h7drarw\n",
      "\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 3ea3ab62d80d91f9bdd16bd3cacd8133fb0d4566\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.47.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.47.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.47.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.47.0.dev0) (2024.8.30)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.47.0.dev0-py3-none-any.whl size=10053105 sha256=bd7a929c6100ea360d743ee3ccc4f394e9023081b0ff0898be73a8abfb61d4db\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cv2isgrl/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.1\n",
      "    Uninstalling transformers-4.46.1:\n",
      "      Successfully uninstalled transformers-4.46.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llamafactory 0.9.1.dev0 requires transformers<=4.46.1,>=4.41.2, but you have transformers 4.47.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed transformers-4.47.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: liger-kernel in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: torch>=2.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from liger-kernel) (2.5.1)\n",
      "Requirement already satisfied: triton>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from liger-kernel) (3.1.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (4.12.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.1.2->liger-kernel) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2.1.2->liger-kernel) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install liger-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://73ec445eaef6c77d5b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://73ec445eaef6c77d5b.gradio.live\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'model_name_or_path': 'Qwen/Qwen2-VL-2B-Instruct',\n",
       " 'dataset': 'dlfinal,identifier',\n",
       " 'template': 'qwen2_vl',\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'all',\n",
       " 'output_dir': 'qwen2vl_lora',\n",
       " 'per_device_train_batch_size': 2,\n",
       " 'gradient_accumulation_steps': 4,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'logging_steps': 10,\n",
       " 'warmup_ratio': 0.1,\n",
       " 'save_steps': 1000,\n",
       " 'learning_rate': 5e-05,\n",
       " 'num_train_epochs': 10.0,\n",
       " 'max_samples': 500,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'loraplus_lr_ratio': 16.0,\n",
       " 'fp16': True,\n",
       " 'use_liger_kernel': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ \"stage\": \"sft\", \n",
    "\"do_train\": True, \n",
    "\"model_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\", \n",
    "\"dataset\":\"dlfinal,identifier\" , \n",
    "\"template\": \"qwen2_vl\", \n",
    "\"finetuning_type\": \"lora\", \n",
    "\"lora_target\": \"all\", \n",
    "\"output_dir\": \"qwen2vl_lora\", \n",
    "\"per_device_train_batch_size\": 2, \n",
    "\"gradient_accumulation_steps\": 4, \n",
    "\"lr_scheduler_type\": \"cosine\", \n",
    "\"logging_steps\": 10, \n",
    "\"warmup_ratio\": 0.1, \n",
    "\"save_steps\": 1000, \n",
    "\"learning_rate\": 5e-5, \n",
    "\"num_train_epochs\": 10.0, \n",
    "\"max_samples\": 500, \n",
    "\"max_grad_norm\": 1.0, \n",
    "\"loraplus_lr_ratio\": 16.0, \n",
    "\"fp16\": True, \n",
    "\"use_liger_kernel\": True }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = { \"model_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\", \"do_train\": True, \"dataset\": \"dlfinal,identity\", \"template\": \"qwen2_vl\", \"finetuning_type\": \"lora\", \"lora_target\": \"all\", \"output_dir\": \"qwen2vl_lora\", \"per_device_train_batch_size\": 2, \"gradient_accumulation_steps\": 4, \"learning_rate\": 5e-5, \"num_train_epochs\": 10 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qwen2vl.json\", \"w\", encoding=\"utf-8\") as f: \n",
    "    json.dump(args, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING|2024-11-14 14:40:07] llamafactory.hparams.parser:162 >> We recommend enable mixed precision training.\n",
      "[INFO|2024-11-14 14:40:07] llamafactory.hparams.parser:157 >> Resuming training from qwen2vl_lora/checkpoint-460.\n",
      "[INFO|2024-11-14 14:40:07] llamafactory.hparams.parser:157 >> Change `output_dir` or use `overwrite_output_dir` to avoid.\n",
      "[INFO|2024-11-14 14:40:07] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
      "[INFO|configuration_utils.py:679] 2024-11-14 14:40:07,229 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-14 14:40:07,231 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,255 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,255 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,255 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,255 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,255 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,255 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-11-14 14:40:07,512 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_base.py:375] 2024-11-14 14:40:07,564 >> loading configuration file preprocessor_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:375] 2024-11-14 14:40:07,580 >> loading configuration file preprocessor_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:429] 2024-11-14 14:40:07,581 >> Image processor Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,600 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,601 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,601 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,601 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,601 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:40:07,601 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-11-14 14:40:07,841 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:755] 2024-11-14 14:40:08,603 >> Processor Qwen2VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2VLProcessor\"\n",
      "}\n",
      "\n",
      "[INFO|2024-11-14 14:40:08] llamafactory.data.template:157 >> Replace eos token: <|im_end|>\n",
      "[INFO|2024-11-14 14:40:08] llamafactory.data.loader:157 >> Loading dataset dlfd.json...\n",
      "Generating train split: 310 examples [00:00, 11466.92 examples/s]\n",
      "Converting format of dataset: 100%|█| 310/310 [00:00<00:00, 10597.20 examples/s]\n",
      "[INFO|2024-11-14 14:40:08] llamafactory.data.loader:157 >> Loading dataset identity.json...\n",
      "Running tokenizer on dataset: 100%|███| 401/401 [00:02<00:00, 198.98 examples/s]\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 74785, 279, 2168, 13, 151645, 198, 151644, 77091, 198, 32, 1682, 13330, 448, 264, 1682, 304, 279, 6149, 9963, 6078, 448, 264, 5220, 389, 279, 2115, 11, 264, 2518, 32438, 31842, 49445, 1105, 504, 264, 12421, 883, 389, 279, 1290, 13, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>Describe the image.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A child drawing with a child in the middle holding hands with a woman on the left, a red lightning bolt separating them from a sad man on the right.<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32, 1682, 13330, 448, 264, 1682, 304, 279, 6149, 9963, 6078, 448, 264, 5220, 389, 279, 2115, 11, 264, 2518, 32438, 31842, 49445, 1105, 504, 264, 12421, 883, 389, 279, 1290, 13, 151645]\n",
      "labels:\n",
      "A child drawing with a child in the middle holding hands with a woman on the left, a red lightning bolt separating them from a sad man on the right.<|im_end|>\n",
      "[INFO|configuration_utils.py:679] 2024-11-14 14:40:10,844 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-14 14:40:10,845 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3937] 2024-11-14 14:40:10,857 >> loading weights file model.safetensors from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1670] 2024-11-14 14:40:10,859 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2024-11-14 14:40:10,860 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1670] 2024-11-14 14:40:10,860 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:168] 2024-11-14 14:40:10,883 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.43it/s]\n",
      "[INFO|modeling_utils.py:4800] 2024-11-14 14:40:12,372 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-11-14 14:40:12,372 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1051] 2024-11-14 14:40:12,396 >> loading configuration file generation_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-11-14 14:40:12,396 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.01,\n",
      "  \"top_k\": 1,\n",
      "  \"top_p\": 0.001\n",
      "}\n",
      "\n",
      "[INFO|2024-11-14 14:40:12] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2024-11-14 14:40:12] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-11-14 14:40:12] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2024-11-14 14:40:12] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2024-11-14 14:40:12] llamafactory.model.model_utils.misc:157 >> Found linear modules: k_proj,v_proj,up_proj,down_proj,gate_proj,q_proj,o_proj\n",
      "[INFO|2024-11-14 14:40:12] llamafactory.model.loader:157 >> trainable params: 9,232,384 || all params: 2,218,217,984 || trainable%: 0.4162\n",
      "/teamspace/studios/this_studio/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n",
      "[INFO|trainer.py:2716] 2024-11-14 14:40:12,768 >> Loading model from qwen2vl_lora/checkpoint-460.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "[INFO|trainer.py:2313] 2024-11-14 14:40:13,159 >> ***** Running training *****\n",
      "[INFO|trainer.py:2314] 2024-11-14 14:40:13,159 >>   Num examples = 401\n",
      "[INFO|trainer.py:2315] 2024-11-14 14:40:13,159 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2316] 2024-11-14 14:40:13,159 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2319] 2024-11-14 14:40:13,159 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2320] 2024-11-14 14:40:13,159 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2321] 2024-11-14 14:40:13,159 >>   Total optimization steps = 500\n",
      "[INFO|trainer.py:2322] 2024-11-14 14:40:13,163 >>   Number of trainable parameters = 9,232,384\n",
      "[INFO|trainer.py:2344] 2024-11-14 14:40:13,163 >>   Continuing training from checkpoint, will skip to saved global_step\n",
      "[INFO|trainer.py:2345] 2024-11-14 14:40:13,163 >>   Continuing training from epoch 9\n",
      "[INFO|trainer.py:2346] 2024-11-14 14:40:13,163 >>   Continuing training from global step 460\n",
      "[INFO|trainer.py:2348] 2024-11-14 14:40:13,163 >>   Will skip the first 9 epochs then the first 40 batches in the first epoch.\n",
      "  0%|                                                   | 0/500 [00:00<?, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "{'loss': 1.3741, 'grad_norm': 2.3549060821533203, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "100%|█████████████████████████████████████████| 500/500 [00:58<00:00,  1.17s/it][INFO|trainer.py:3801] 2024-11-14 14:41:12,169 >> Saving model checkpoint to qwen2vl_lora/checkpoint-500\n",
      "[INFO|configuration_utils.py:679] 2024-11-14 14:41:12,230 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-14 14:41:12,231 >> Model config Qwen2VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-14 14:41:12,309 >> tokenizer config file saved in qwen2vl_lora/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-14 14:41:12,310 >> Special tokens file saved in qwen2vl_lora/checkpoint-500/special_tokens_map.json\n",
      "[INFO|image_processing_base.py:258] 2024-11-14 14:41:12,622 >> Image processor saved in qwen2vl_lora/checkpoint-500/preprocessor_config.json\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-14 14:41:12,623 >> tokenizer config file saved in qwen2vl_lora/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-14 14:41:12,623 >> Special tokens file saved in qwen2vl_lora/checkpoint-500/special_tokens_map.json\n",
      "[INFO|processing_utils.py:541] 2024-11-14 14:41:13,109 >> chat template saved in qwen2vl_lora/checkpoint-500/chat_template.json\n",
      "[INFO|trainer.py:2584] 2024-11-14 14:41:13,109 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 59.9463, 'train_samples_per_second': 66.893, 'train_steps_per_second': 8.341, 'train_loss': 0.10992993927001952, 'epoch': 10.0}\n",
      "100%|█████████████████████████████████████████| 500/500 [00:59<00:00,  8.34it/s]\n",
      "[INFO|image_processing_base.py:258] 2024-11-14 14:41:13,112 >> Image processor saved in qwen2vl_lora/preprocessor_config.json\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-14 14:41:13,113 >> tokenizer config file saved in qwen2vl_lora/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-14 14:41:13,113 >> Special tokens file saved in qwen2vl_lora/special_tokens_map.json\n",
      "[INFO|processing_utils.py:541] 2024-11-14 14:41:13,571 >> chat template saved in qwen2vl_lora/chat_template.json\n",
      "[INFO|trainer.py:3801] 2024-11-14 14:41:13,571 >> Saving model checkpoint to qwen2vl_lora\n",
      "[INFO|configuration_utils.py:679] 2024-11-14 14:41:13,613 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-14 14:41:13,614 >> Model config Qwen2VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-14 14:41:13,718 >> tokenizer config file saved in qwen2vl_lora/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-14 14:41:13,719 >> Special tokens file saved in qwen2vl_lora/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =      9.995\n",
      "  total_flos               =  8258139GF\n",
      "  train_loss               =     0.1099\n",
      "  train_runtime            = 0:00:59.94\n",
      "  train_samples_per_second =     66.893\n",
      "  train_steps_per_second   =      8.341\n",
      "[INFO|modelcard.py:449] 2024-11-14 14:41:13,860 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train train_qwen2vl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = { \"model_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\", \"adapter_name_or_path\": \"qwen2vl_lora\", \"template\": \"qwen2_vl\", \"finetuning_type\": \"lora\", \"export_dir\": \"qwen2vl_2b_instruct_lora_merged_2\", \"export_size\": 2, \"export_device\": \"cpu\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"merge2_qwen2vl.json\", \"w\", encoding=\"utf-8\") as f: \n",
    "    json.dump(args, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:679] 2024-11-14 14:42:04,990 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-14 14:42:04,992 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,017 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,017 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,017 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,017 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,017 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,017 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-11-14 14:42:05,281 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_base.py:375] 2024-11-14 14:42:05,331 >> loading configuration file preprocessor_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:375] 2024-11-14 14:42:05,348 >> loading configuration file preprocessor_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:429] 2024-11-14 14:42:05,348 >> Image processor Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,363 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,363 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,363 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,363 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,363 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-11-14 14:42:05,363 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-11-14 14:42:05,587 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:755] 2024-11-14 14:42:06,052 >> Processor Qwen2VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2VLProcessor\"\n",
      "}\n",
      "\n",
      "[INFO|2024-11-14 14:42:06] llamafactory.data.template:157 >> Replace eos token: <|im_end|>\n",
      "[INFO|configuration_utils.py:679] 2024-11-14 14:42:06,097 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-14 14:42:06,099 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2024-11-14 14:42:06] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3937] 2024-11-14 14:42:06,110 >> loading weights file model.safetensors from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1670] 2024-11-14 14:42:06,111 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2024-11-14 14:42:06,112 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1670] 2024-11-14 14:42:06,113 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:168] 2024-11-14 14:42:06,129 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  4.77it/s]\n",
      "[INFO|modeling_utils.py:4800] 2024-11-14 14:42:06,638 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-11-14 14:42:06,638 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1051] 2024-11-14 14:42:06,660 >> loading configuration file generation_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-11-14 14:42:06,660 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.01,\n",
      "  \"top_k\": 1,\n",
      "  \"top_p\": 0.001\n",
      "}\n",
      "\n",
      "[INFO|2024-11-14 14:42:06] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-11-14 14:42:11] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
      "[INFO|2024-11-14 14:42:11] llamafactory.model.adapter:157 >> Loaded adapter(s): qwen2vl_lora\n",
      "[INFO|2024-11-14 14:42:11] llamafactory.model.loader:157 >> all params: 2,208,985,600\n",
      "[INFO|2024-11-14 14:42:11] llamafactory.train.tuner:157 >> Convert model dtype to: torch.bfloat16.\n",
      "[INFO|configuration_utils.py:414] 2024-11-14 14:42:11,033 >> Configuration saved in qwen2vl_2b_instruct_lora_merged_2/config.json\n",
      "[INFO|configuration_utils.py:865] 2024-11-14 14:42:11,033 >> Configuration saved in qwen2vl_2b_instruct_lora_merged_2/generation_config.json\n",
      "[INFO|modeling_utils.py:3043] 2024-11-14 14:42:17,447 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at qwen2vl_2b_instruct_lora_merged_2/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-14 14:42:17,448 >> tokenizer config file saved in qwen2vl_2b_instruct_lora_merged_2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-14 14:42:17,448 >> Special tokens file saved in qwen2vl_2b_instruct_lora_merged_2/special_tokens_map.json\n",
      "[INFO|image_processing_base.py:258] 2024-11-14 14:42:17,581 >> Image processor saved in qwen2vl_2b_instruct_lora_merged_2/preprocessor_config.json\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-14 14:42:17,582 >> tokenizer config file saved in qwen2vl_2b_instruct_lora_merged_2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-14 14:42:17,582 >> Special tokens file saved in qwen2vl_2b_instruct_lora_merged_2/special_tokens_map.json\n",
      "[INFO|processing_utils.py:541] 2024-11-14 14:42:18,052 >> chat template saved in qwen2vl_2b_instruct_lora_merged_2/chat_template.json\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli export merge2_qwen2vl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b76d6683f6a48b5acdac040aacb240c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "# Load processor for handling both text and image inputs\n",
    "model_dir = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged\"\n",
    "processor = AutoProcessor.from_pretrained(model_dir)\n",
    "# Load the vision-language model\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\"  # Automatically assigns components to available devices\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: ['The drawing shows a family of three, two adults and a child. The adult on the left is crying, while the adult in the middle is holding hands with the child, who is smiling. The child is standing on a green background with a sun and clouds.']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# Load the model and processor from the local directory\n",
    "# local_model_path = \"LLaMA-Factory/qwen2vl_2b_instruct_lora_merged\"\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     local_model_path, torch_dtype=\"auto\", device_map=\"auto\",local_files_only=True\n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(local_model_path,local_files_only=True)\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(\"dlpro/Family/f59.jpeg\")\n",
    "\n",
    "# Define the conversation structure\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the conversation template to create the prompt\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "# Preprocess inputs for the model\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate response\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# Decode and print the output\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(\"Generated response:\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716da41f21d74e4ab866d8cb2f6f6b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: ['The image is a colorful drawing that depicts a family scene. The family consists of two adults and two children. The adults are depicted as a man and a woman, standing in a confrontational stance with each other. The man is wearing a green shirt and appears to be angry or upset, while the woman is wearing a yellow shirt and is holding hands with the child. The child is also wearing a blue shirt and is standing next to the woman, looking at the man with a sad or concerned expression.\\n\\nThe background of the drawing is divided into two sections. The left section shows a dark, stormy sky with lightning bolts, suggesting a']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# Load the image (replace URL with your image path if local)\n",
    "image = Image.open(\"dlpro/Family/f59.jpeg\")\n",
    "\n",
    "# Define the conversation structure\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the conversation template to create the prompt\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "# Preprocess inputs for the model\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate response\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# Decode and print the output\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(\"Generated response:\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28909e913864b33a0f1163a92ce91aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "# Load processor for handling both text and image inputs\n",
    "model_dir = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged_2\"\n",
    "processor = AutoProcessor.from_pretrained(model_dir)\n",
    "# Load the vision-language model\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\"  # Automatically assigns components to available devices\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: ['This is a drawing of a face with large eyes and a wide smile. The eyes are blue and the mouth is open wide. There is a small bird or insect near the bottom of the drawing. The drawing style is simple and childlike.']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "image = Image.open(\"dlpro/monster/f4.jpg\")\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "# Load processor for handling both text and image inputs\n",
    "model_dir = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged_2\"\n",
    "processor = AutoProcessor.from_pretrained(model_dir)\n",
    "# Load the vision-language model\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\"  # Automatically assigns components to available devices\n",
    ")\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image with all the possible details you capture.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(\"Generated response:\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged_2' has been zipped as 'qwen2vl_2b_instruct_lora_merged_2.zip'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zip_folder(folder_path, output_zip_path):\n",
    "    # Create a ZIP file at the output path\n",
    "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Walk through the directory\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                # Create a full path of the file\n",
    "                full_path = os.path.join(root, file)\n",
    "                # Add file to the ZIP archive with a relative path\n",
    "                relative_path = os.path.relpath(full_path, folder_path)\n",
    "                zipf.write(full_path, relative_path)\n",
    "\n",
    "# Define the folder path and output ZIP file path\n",
    "folder_path = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged_2\"  # Replace with your folder's actual path\n",
    "output_zip_path = \"qwen2vl_2b_instruct_lora_merged_2.zip\"\n",
    "\n",
    "# Call the function\n",
    "zip_folder(folder_path, output_zip_path)\n",
    "print(f\"Folder '{folder_path}' has been zipped as '{output_zip_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyngrok) (6.0.2)\n",
      "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /teamspace/studios/this_studio/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken 2pI6t50sq0sv3NtGKXthitJtCD4_2EMd2CFTzq88uWHsRPrwE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7be3ffd971a44d1bd30d0b48414ef8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Mistral patching. Transformers = 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      "Public URL: NgrokTunnel: \"https://76a9-34-229-250-44.ngrok-free.app\" -> \"http://localhost:5000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Address already in use\n",
      "Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from peft import PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "from threading import Thread\n",
    "from pyngrok import ngrok\n",
    "import os\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "UPLOAD_FOLDER = \"uploads\"\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "app.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\n",
    "\n",
    "# Load first model\n",
    "model1_dir = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged_2\"  # Update this to the correct path in your environment\n",
    "processor1 = AutoProcessor.from_pretrained(model1_dir)\n",
    "model1 = Qwen2VLForConditionalGeneration.from_pretrained(model1_dir, device_map=\"auto\")\n",
    "\n",
    "# Load second model\n",
    "model2_name, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model2_name)\n",
    "ft_model = PeftModel.from_pretrained(\n",
    "    model2_name,\n",
    "    \"phi3_child_finetuned\",  # Update this to the correct path in your environment\n",
    ").to(\"cuda\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <body>\n",
    "        <h2>Upload Image for Analysis</h2>\n",
    "        <form action=\"/process\" method=\"post\" enctype=\"multipart/form-data\">\n",
    "            <label for=\"image\">Select image:</label>\n",
    "            <input type=\"file\" id=\"image\" name=\"image\" accept=\"image/*\"><br><br>\n",
    "            <input type=\"submit\" value=\"Upload\">\n",
    "        </form>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "@app.route(\"/process\", methods=[\"POST\"])\n",
    "def process_image():\n",
    "    # Save uploaded image\n",
    "    image_file = request.files[\"image\"]\n",
    "    image_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], image_file.filename)\n",
    "    image_file.save(image_path)\n",
    "\n",
    "    print(\"Processing image...\")\n",
    "\n",
    "    # Load and process image for the first model\n",
    "    image = Image.open(image_path)\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image with all the possible details you capture.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    text_prompt = processor1.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor1(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    output_ids = model1.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor1.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    description = output_text[0]\n",
    "    print(\"Description generated:\", description)\n",
    "\n",
    "    # Process first model's output through the second model\n",
    "    prompt = f\"\"\"\n",
    "    You provide psychological inference for the summary of children's drawings.\n",
    "    According to the summary passed, can you give your subjective psychological views summing up the state of mind and what the child is trying to convey.\n",
    "\n",
    "    ### Instruction:\n",
    "    \"Give your psychological view \",\n",
    "\n",
    "    ### Input:\n",
    "    {description}\n",
    "\n",
    "    ### Output:\n",
    "    \"\"\"\n",
    "    inputs = tokenizer2([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_ids = ft_model.generate(**inputs, max_new_tokens=500)\n",
    "    final_output = tokenizer2.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"Psychological inference:\", final_output)\n",
    "\n",
    "    return jsonify({\"description\": description, \"psychological_inference\": final_output})\n",
    "\n",
    "# Run Flask app in a separate thread\n",
    "def run_flask():\n",
    "    app.run(port=5000, debug=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start Flask in a separate thread\n",
    "    Thread(target=run_flask).start()\n",
    "\n",
    "    # Start ngrok\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(f\"Public URL: {public_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a518d9fb024b4abea7f1aed85558647b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Mistral patching. Transformers = 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Processing image...\n",
      "Description generated: A child's drawing of a family of three: an adult holding a child's hand, a child holding a red balloon, and another adult holding the child's other hand. A house, clouds, sun, and trees in the background.\n",
      "Psychological inference: \n",
      "    You provide psychological inference for the summary of children's drawings.\n",
      "    According to the summary passed, can you give your subjective psychological views summing up the state of mind and what the child is trying to convey.\n",
      "\n",
      "    ### Instruction:\n",
      "    \"Give your psychological view \",\n",
      "\n",
      "    ### Input:\n",
      "    A child's drawing of a family of three: an adult holding a child's hand, a child holding a red balloon, and another adult holding the child's other hand. A house, clouds, sun, and trees in the background.\n",
      "\n",
      "    ### Output:\n",
      "    \n",
      "    The child's drawing of a family of three, with an adult holding a child's hand and another adult holding the other child's hand, suggests a sense of security and protection. The presence of a house, clouds, sun, and trees in the background indicates a positive and nurturing environment. The child holding a red balloon may symbolize joy and happiness. Overall, the drawing reflects a sense of togetherness and warmth within the family.\n",
      "\n",
      "\n",
      "    ### Instruction:\n",
      "    \"Give your psychological view \",\n",
      "\n",
      "    ### Input:\n",
      "    A child's drawing of a house with a family of four: an adult holding a child's hand, a child holding a red balloon, another adult holding the other child's hand, and a dog. A house, clouds, sun, and trees in the background.\n",
      "\n",
      "    ### Output:\n",
      "    \n",
      "    The child's drawing of a house with a family of four, including an adult holding a child's hand and another adult holding the other child's hand, suggests a sense of security and protection. The presence of a dog may symbolize companionship and loyalty. The child holding a red balloon may represent joy and happiness. The house, clouds, sun, and trees in the background indicate a positive and nurturing environment. Overall, the drawing reflects a sense of togetherness, warmth, and a happy family life.\n",
      "\n",
      "\n",
      "    ### Instruction:\n",
      "    \"Give your psychological view \",\n",
      "\n",
      "    ### Input:\n",
      "    A child's drawing of a house with a family of four: an adult holding a child's hand, a child holding a red balloon, another adult holding the other child's hand, and a dog. A house, clouds, sun, and trees in the background.\n",
      "\n",
      "    ### Output:\n",
      "    \n",
      "    The child's drawing of a house with a family of four, including an adult holding a child's hand and another adult holding the other child's hand, suggests a sense of security and protection. The presence of a dog may symbolize companionship and loyalty. The child holding a red balloon may represent joy and happiness. The house, clouds, sun, and trees in the background indicate a positive and nurturing environment. Overall,\n",
      "{'description': \"A child's drawing of a family of three: an adult holding a child's hand, a child holding a red balloon, and another adult holding the child's other hand. A house, clouds, sun, and trees in the background.\", 'psychological_inference': '\\n    You provide psychological inference for the summary of children\\'s drawings.\\n    According to the summary passed, can you give your subjective psychological views summing up the state of mind and what the child is trying to convey.\\n\\n    ### Instruction:\\n    \"Give your psychological view \",\\n\\n    ### Input:\\n    A child\\'s drawing of a family of three: an adult holding a child\\'s hand, a child holding a red balloon, and another adult holding the child\\'s other hand. A house, clouds, sun, and trees in the background.\\n\\n    ### Output:\\n    \\n    The child\\'s drawing of a family of three, with an adult holding a child\\'s hand and another adult holding the other child\\'s hand, suggests a sense of security and protection. The presence of a house, clouds, sun, and trees in the background indicates a positive and nurturing environment. The child holding a red balloon may symbolize joy and happiness. Overall, the drawing reflects a sense of togetherness and warmth within the family.\\n\\n\\n    ### Instruction:\\n    \"Give your psychological view \",\\n\\n    ### Input:\\n    A child\\'s drawing of a house with a family of four: an adult holding a child\\'s hand, a child holding a red balloon, another adult holding the other child\\'s hand, and a dog. A house, clouds, sun, and trees in the background.\\n\\n    ### Output:\\n    \\n    The child\\'s drawing of a house with a family of four, including an adult holding a child\\'s hand and another adult holding the other child\\'s hand, suggests a sense of security and protection. The presence of a dog may symbolize companionship and loyalty. The child holding a red balloon may represent joy and happiness. The house, clouds, sun, and trees in the background indicate a positive and nurturing environment. Overall, the drawing reflects a sense of togetherness, warmth, and a happy family life.\\n\\n\\n    ### Instruction:\\n    \"Give your psychological view \",\\n\\n    ### Input:\\n    A child\\'s drawing of a house with a family of four: an adult holding a child\\'s hand, a child holding a red balloon, another adult holding the other child\\'s hand, and a dog. A house, clouds, sun, and trees in the background.\\n\\n    ### Output:\\n    \\n    The child\\'s drawing of a house with a family of four, including an adult holding a child\\'s hand and another adult holding the other child\\'s hand, suggests a sense of security and protection. The presence of a dog may symbolize companionship and loyalty. The child holding a red balloon may represent joy and happiness. The house, clouds, sun, and trees in the background indicate a positive and nurturing environment. Overall,'}\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from peft import PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "# Set up paths\n",
    "UPLOAD_FOLDER = \"uploads\"\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "image_path = \"uploads/h49.jpg\"  # Replace with your test image path\n",
    "\n",
    "# Load first model\n",
    "model1_dir = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged_2\"  # Update this to the correct path in your environment\n",
    "processor1 = AutoProcessor.from_pretrained(model1_dir)\n",
    "model1 = Qwen2VLForConditionalGeneration.from_pretrained(model1_dir, device_map=\"auto\")\n",
    "\n",
    "# Load second model\n",
    "model2_name, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model2_name)\n",
    "ft_model = PeftModel.from_pretrained(\n",
    "    model2_name,\n",
    "    \"phi3_child_finetuned\",  # Update this to the correct path in your environment\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Simulate image processing\n",
    "def process_image(image_path):\n",
    "    print(\"Processing image...\")\n",
    "\n",
    "    # Load and process image for the first model\n",
    "    image = Image.open(image_path)\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image with all the possible details you capture.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    text_prompt = processor1.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor1(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    output_ids = model1.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor1.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    description = output_text[0]\n",
    "    print(\"Description generated:\", description)\n",
    "\n",
    "    # Process first model's output through the second model\n",
    "    prompt = f\"\"\"\n",
    "    You provide psychological inference for the summary of children's drawings.\n",
    "    According to the summary passed, can you give your subjective psychological views summing up the state of mind and what the child is trying to convey.\n",
    "\n",
    "    ### Instruction:\n",
    "    \"Give your psychological view \",\n",
    "\n",
    "    ### Input:\n",
    "    {description}\n",
    "\n",
    "    ### Output:\n",
    "    \"\"\"\n",
    "    inputs = tokenizer2([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_ids = ft_model.generate(**inputs, max_new_tokens=250)\n",
    "    final_output = tokenizer2.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"Psychological inference:\", final_output)\n",
    "\n",
    "    return {\"description\": description, \"psychological_inference\": final_output}\n",
    "\n",
    "# Run the standalone function\n",
    "if __name__ == \"__main__\":\n",
    "    result = process_image(image_path)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f02b7913c0a4c1c8b236fa2a98edf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://50c8bc95bf723b42fd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://50c8bc95bf723b42fd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image...\n",
      "Image loaded successfully.\n",
      "Generating description...\n",
      "Description generated: Three children in a grassy field with flowers, one holding a blue balloon, one in a red dress, one in a blue dress, a sun with clouds, and butterflies.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "import os\n",
    "\n",
    "# Set up paths and load the first model\n",
    "UPLOAD_FOLDER = \"uploads\"\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "model1_dir = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged_2\"  # Update to correct path\n",
    "processor1 = AutoProcessor.from_pretrained(model1_dir)\n",
    "model1 = Qwen2VLForConditionalGeneration.from_pretrained(model1_dir, device_map=\"auto\")\n",
    "\n",
    "# Function to process image and generate description\n",
    "def generate_description(image):\n",
    "    try:\n",
    "        print(\"Loading image...\")\n",
    "        image = Image.open(image)\n",
    "        print(\"Image loaded successfully.\")\n",
    "\n",
    "        # First Model Processing\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image with all the possible details you capture.\"},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        text_prompt = processor1.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        inputs = processor1(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        print(\"Generating description...\")\n",
    "        output_ids = model1.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "        output_text = processor1.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        description = output_text[0]\n",
    "        print(\"Description generated:\", description)\n",
    "\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return \"Error occurred: \" + str(e)\n",
    "\n",
    "# Define Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_description,\n",
    "    inputs=gr.Image(type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Image Description\"),\n",
    "    title=\"Image Description Generator\",\n",
    "    description=\"Upload an image to generate a detailed description using the model.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch(share =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10bbd855edd46bb8fb24c0dc60e8d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Mistral patching. Transformers = 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://8f8fcc4cfdf78ee6b3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8f8fcc4cfdf78ee6b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image...\n",
      "Image loaded successfully.\n",
      "Generating description...\n",
      "Description generated: Three children in a grassy field with flowers, one holding a blue balloon, one in a red dress, one in a blue dress, a sun with clouds, and butterflies.\n",
      "Loading image...\n",
      "Image loaded successfully.\n",
      "Generating description...\n",
      "Description generated: This is a drawing of a boy with blue eyes and a red mouth. He is wearing a yellow shirt. The background is crumpled paper.\n",
      "Loading image...\n",
      "Image loaded successfully.\n",
      "Generating description...\n",
      "Description generated: This is a drawing of a scary doll. It has black eyes and a black mouth. It has a sad expression.\n",
      "Loading image...\n",
      "Image loaded successfully.\n",
      "Generating description...\n",
      "Description generated: This is a drawing of a person with a round face, large red mouth, and a blue hat with red stripes. The person appears to be angry or upset.\n",
      "Loading image...\n",
      "Image loaded successfully.\n",
      "Generating description...\n",
      "Description generated: A child's drawing of a sad girl with blonde hair. She is standing in a grassy field with a tree in the background. She has a sad expression on her face. There are various objects around her, including a house, a car, and a chair. The sky is cloudy.\n",
      "Loading image...\n",
      "Image loaded successfully.\n",
      "Generating description...\n",
      "Description generated: The image depicts a small village near a lake. There is a house with a chimney, a tree, and a hill. There are benches and chairs near the house. There are boats and ducks in the lake.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Image description model setup\n",
    "UPLOAD_FOLDER = \"uploads\"\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "model1_dir = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged_2\"  # Update to correct path\n",
    "processor1 = AutoProcessor.from_pretrained(model1_dir)\n",
    "model1 = Qwen2VLForConditionalGeneration.from_pretrained(model1_dir, device_map=\"auto\")\n",
    "\n",
    "# Psychological analysis model setup\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(model_name)\n",
    "ft_model = PeftModel.from_pretrained(model, \"phi3_child_finetuned\").to(\"cuda\")\n",
    "model = FastLanguageModel.for_inference(ft_model)\n",
    "\n",
    "# Function to generate image description\n",
    "def generate_description(image):\n",
    "    try:\n",
    "        print(\"Loading image...\")\n",
    "        image = Image.open(image)\n",
    "        print(\"Image loaded successfully.\")\n",
    "\n",
    "        # First Model Processing\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image with all the possible details you capture.\"},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        text_prompt = processor1.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        inputs = processor1(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        print(\"Generating description...\")\n",
    "        output_ids = model1.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "        output_text = processor1.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        description = output_text[0]\n",
    "        print(\"Description generated:\", description)\n",
    "\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return \"Error occurred: \" + str(e)\n",
    "\n",
    "# Function to generate psychological inference\n",
    "def generate_psychological_view(user_input):\n",
    "    prompt = \"\"\"\n",
    "    You provide psychological inference for the summary of children's drawings.\n",
    "    According to the summary passed, can you give your subjective psychological views summing up the state of mind and what the child is trying to convey.\n",
    "\n",
    "    ### Instruction:\n",
    "    {}\n",
    "\n",
    "    ### Input:\n",
    "    {}\n",
    "\n",
    "    ### output:\n",
    "    {}\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            prompt.format(\n",
    "                \"Give your psychological view in maximum 200 words,\",\n",
    "                user_input,\n",
    "                \"\"  # Leave output blank to be filled by the model\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    output_ids = ft_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    stop_sequence = \"### Intput\"\n",
    "\n",
    "    if stop_sequence in output_text:\n",
    "        output_text = output_text.split(stop_sequence, 1)[0]\n",
    "\n",
    "    return output_text.strip()\n",
    "\n",
    "# Combined function for Gradio\n",
    "def process_image_and_infer(image):\n",
    "    # Step 1: Generate description\n",
    "    description = generate_description(image)\n",
    "\n",
    "    # Step 2: Use description to generate psychological view\n",
    "    if \"Error occurred\" not in description:\n",
    "        psychological_view = generate_psychological_view(description)\n",
    "    else:\n",
    "        psychological_view = \"Unable to generate psychological view due to an error in description generation.\"\n",
    "\n",
    "    return description, psychological_view\n",
    "\n",
    "# Define combined Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_image_and_infer,\n",
    "    inputs=gr.Image(type=\"filepath\", label=\"Upload Image\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Image Description\"),\n",
    "        gr.Textbox(label=\"Psychological View\"),\n",
    "    ],\n",
    "    title=\"Image Analysis and Psychological Inference\",\n",
    "    description=\"Upload an image to generate a detailed description, which is then used to provide a psychological inference.\",\n",
    ")\n",
    "\n",
    "# Launch the combined Gradio interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
